---
layout:     post                    
title:      自编码器               
subtitle:   自编码器的种类介绍
date:       2019-01-19             
author:     xinxin                     
header-img: img/post-bg-coffee.jpg    
catalog: true                      
mathjax: true
tags:                              
    - 特征提取 降维 信息检索 深度学习
---

# 自编码器

在网上一直在搜自编码器的相关资料，但好多看不懂，可能是自己水平限制吧，毕竟对自编码器什么都不懂。经过自己这几天搜集资料，希望能够写一篇相对完善的关于自编码器的资料，希望能有小白看过之后对自编码器有一个初步的了解。

自编码器是神经网络的一种，经过训练后能够将输入复制到输出。该网络可以看到由两部分组成：一个由函数h=f(x)表示的编码器和一个生成重构的解码器r=g(h)。这个可能很多人认为没有什么作用，但是我们需要的就是对自编码器进行训练得到中间层数据，多应用于降维。这个需要限制h的维度比x小，这种编码维度小于输入维度的自编码器成为欠完备自编码器。

## 1.欠完备自编码器

欠完备自编码器的学习过程可以简单的描述为最小化一个损失函数L(x,g(f(x)))，其中L是一个损失函数。当编码器是线性且L是均方误差，欠完备的自编码器就会学习出与PCA相同的生成子空间。这种情况下，自编码器在训练来执行复制任务的同时学到了训练数据的主元子空间。因此拥有非线性编码器函数f和非线性解码器函数g的自编码器能够学习出更强大的PCA非线性推广。

## 2.正则自编码器

理想情况下，根据要建模的数据分布的复杂性，选择合适的编码维数和编码器、解码器容量，就可以成功训练任意架构的自编码器。正则自编码器就可以担当重任。正则编码器使用的损失函数可以鼓励模型学习其他特性（除了将模型复制到输出），而不必限制使用浅层的编码器和解码器以及小的编码维数来限制模型的容量。这些特性包括稀疏表示、表示的小导数以及对噪声或输入缺失的鲁棒性。即使模型容量大到足以学习一个无意义的恒等函数，非线性且过完备的正则自编码器仍然能够从数据中学到一些关于数据分布的有用信息。

强调与自编码器联系的两个生成式建模方法是Helmholtz机的衍生模型，如变分自编码器和生成随机网络。这些变种自编码器能够学习出高容量且过完备的模型，进而发现输入数据中有用的结构信息，并且也无须对模型进行正则化。

### 2.1稀疏自编码器

稀疏自编码器简单的在训练时结合编码层的系数惩罚Ω(h)和重构误差：

$$
{\rm{L}}(x,g(f(x))) + \Omega (h)
$$

其中g(h)是解码器的输出，通常h是编码器的输出，即h=f(x)。

稀疏自编码器一般用来学习特征，一边用于像分类这样的任务。这里可以将惩罚项Ω(h)视为加到前馈网络的正则项，这个前馈网络的主要任务是将输入复制到输出（无监督学习的目标），并尽可能地根据这些稀疏特征执行一些监督学习任务，这样的话能得到学习有用特征的模型。这里的正则项取决于数据，而不是一个先验，不过仍然可以认为是这些正则项隐式地表达了对函数的偏好。

将稀疏自编码器视为是对带有潜变量的生成模型的近似最大似然训练，则有

$$
{\rm{p}}_{\bmod {\rm{el}}} (x,h) = p_{\bmod el} (h)p_{\bmod el} (x|h)
$$


将$$p_{\bmod el} (h)$$视为模型关于潜变量的先验分布，表示模型看到x的信念先验。注意此处的先验和之前的先验并不一样，之前指的是看到数据前就对模型参数的先验进行编码。对数似然函数可分解为：

$$
\log p_{\bmod el} (x) = \log \sum\limits_h {p_{\bmod el} (h,x)} 
$$

可以认为自编码器使用一个高似然值h的点估计近似这个总和。因此估计这个总和就转化为寻找一个高似然值h。由于

$$
\log p_{\bmod el} (h,x) = \log p_{\bmod el} (h) + \log p_{\bmod el} (x|h)
$$

$$\log p_{\bmod el} (h)$$项能被稀疏诱导。如Laplace先验，

$$
p_{\bmod el} (h_i ) = \frac{\lambda }{2}e^{ - \lambda \left| {h_i } \right|} 
$$

对应于绝对值稀疏惩罚。将对数先验表示为绝对值惩罚，得到：

$$
\Omega (h) = \lambda \sum\limits_i {\left| {h_i } \right|} 
$$

$$
 - \log p_{\bmod el} (h) = \sum\limits_i {(\lambda \left| {h_i } \right| - \log \frac{\lambda }{2})}  = \Omega (h) + const
$$

这里的常数项只和λ有关。从稀疏性导致$$p_{\bmod el} (h)$$学习成近似最大似然的结果看，稀疏惩罚完全不是一个正则项。这仅仅影响模型关于潜变量的分布。同时这也是为什么自编码器学习到的特征是有用的另一个解释：它们描述的潜变量可以解释输入。

### 2.2去噪自编码器

除了像之前稀疏自编码器向代价函数增加一个惩罚项，也可以通过改变重构误差项来获得一个能学到有用信息的自编码器。
去噪自编码器（denoising autoencoder,DAE）是一类接受损坏数据作为输入，并训练来预测原始未被损坏数据作为输出的自编码器。损失函数为
$$
L(x,g(f(\tilde x)))
$$,即

$$
L =  - \log p_{decoder} (x|h = f(\tilde x))
$$

DAE的训练过程如下图所示。引入一个损坏过程$$
C({\rm{\tilde x|x}})
$$,这个条件分布代表给定数据样本x产生损坏样本 的概率。自编码器则根据以下过程，从训练数据对$$(x,\tilde x)$$中学习重构分布$$
p_{reconstruct} ({\rm{x|\tilde x)}}
$$:

1) 从训练数据中采集一个训练样本 ；

2) 从
$$
C({\rm{\tilde x}}|{\rm{x}} = x)
$$
采一个损坏样本;

3) 将$$(x,\tilde x)$$作为训练样本来估计自编码器的重构分布$$
p_{reconstruct} (x|\tilde x) = p_{decoder} (x|h)$$
，其中$$h$$是编码器$$f(\tilde x)$$的输出,$$p_{decoder}$$
根据解码函数$$g(h)$$定义。

![.](/img/post-DAEdaijia.jpg)

DAE的一个重要特性就是DAE的训练准则(条件高斯
$$p(x|h)$$
)能让自编码器学到能估计数据分布得分的向量场$$(g(f(x)) - x)$$。在一个确定的噪声水平下，正则化的得分匹配不是一致估计量，相反它会恢复分布的一个模糊版本，然而当噪声水平趋向于0且训练样本数趋向于无穷时，一致性就会恢复。其中得分是一个特定的梯度场：$$\nabla _x \log p(x)$$。

对于连续的$$x$$,高斯损坏和重构分布的去噪准则得到的得分估计适用于一般编码器和解码器的参数化。这意味着一个使用平方误差准则

$$
\left\| {g(f(\tilde x)) - x} \right\|^2 
$$

和噪声方差为$$\sigma ^2 $$的损坏

$$
C({\rm{\tilde x}} = \tilde x|x) = N(\tilde x;\mu  = x,\Sigma  = \sigma ^2 I)
$$

的通用编码器-解码器架构可以用来估计训练得分。

### 2.3收缩自编码器

收缩自编码器在编码 的基础上添加了显示的正则项，鼓励$$h = f(x)$$
的导数尽可能小：

$$
\Omega (h) = \lambda \left\| {\frac{\partial f(x)}{\partial x}} \right\|_F^2 
$$

惩罚项$$\Omega (h)$$为平方Frobenius范数（元素平方之和），作用于与编码器的函数相关偏导数的Jacobian矩阵。
去噪自编码器和收缩自编码器之间存在一定联系：Alain and Bengio（2013）指出在小高斯噪声的限制下，当重构函数将$$x$$映射到$$
r = g(f(x))
$$时，去噪重构误差与收缩惩罚项是等价的。换句话说，去噪自编码器能抵抗小且有限的输入扰动，而收缩自编码器使特征提取函数能抵抗极小的输入扰动。

## 3.自编码器的深度和表示能力

深度自编码器（编码器至少包含一层额外隐藏层）在给定足够多的隐藏单元的情况下，能以任意精度近似任何从输入到编码的映射。深度可以指数地降低表示某些函数的计算成本。深度也能指数的减少学习一些函数所需的训练数据量。

训练深度自编码器的普遍策略是训练一堆浅层的自编码器来贪心地预训练相应的深度架构。所以即使最终目标是训练深度自编码器，我们也经常会遇到浅层自编码器。

## 4.随机编码器和解码器

自编码器本质上是一个前馈网络，可以使用与传统前馈网络相同的损失函数和输出单元。
为了彻底地与前馈网络相区别，可以将编码函数$$f(x)$$的概念推广为编码分布。任何潜变量模型$$
{\rm{p}}_{\bmod el} (h,x)
$$定义一个随机编码器

$$
p_{encoder} (h|x) = p_{\bmod el} (h|x)
$$

以及一个随机解码器
 
$$
p_{decoder} (x|h) = p_{\bmod el} (x|h)
$$

通常情况下，编码器和解码器的分布没必要是与唯一一个联合分布$$p_{\bmod el} (x,h)$$相容的条件分布。Alain et al（2015）指出，在保证足够容量和样本的情况下，将编码器和解码器作为去噪自编码器训练，能使它们渐进地相容。

## 5.预测稀疏分解

预测稀疏分解是稀疏编码和参数化自编码器的混合模型。参数化编码器被训练为能预测迭代推断的输出。这个模型由一个编码器f(x)和一个解码器g(h)组成，并且都是参数化的。在训练过程中，h由优化算法控制。优化过程是最小化

$$
\left\| {x - g(h)} \right\|^2  + \lambda \left| h \right|_1  + \gamma \left\| {h - f(x)} \right\|^2 
$$

就像稀疏编码，训练算法交替地相对h和模型的参数最小化上述目标。相对h最小化较快，因为f(x)提供h的良好初始值以及损失函数将h约束在f(x)附近。简单的梯度下降算法只需10步左右就能获得理想的h。

## 6.总结

自编码器跟其他很多机器学习算法一样，利用了数据集中在一个低维流形或者一小组这样的流行的思想。流行的一个重要特征就是切平面的集合。d维流形上的一点x，切平面由能张成流行上允许变动的局部方向的d维基向量给出。

所有自编码器的训练过程涉及两种推动力的折衷：

1)学习训练样本x的表示h使得x能通过解码器近似地从h中恢复。

2)满足约束或正则惩罚。这既可以是限制自编码器容量的架构约束，也可以是加入到重构代价的一个正则项。






